{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f7c98a-1c13-4885-b32b-c06fc616d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from model import *\n",
    "import uuid\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a78d4-9c70-4fd6-9ea2-4a6e3e962c36",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccd76f6-898b-4cfa-99f0-2447b6e13291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.model = \"GCN_LC\"\n",
    "        # self.model = \"JKnet_LC\"\n",
    "        # self.model = \"GPRGNN_LC4\"\n",
    "        # self.model = \"SGC\"\n",
    "        # self.model = \"FSGNN\"\n",
    "        self.datastr = \"flickr\"\n",
    "        # self.datastr = \"reddit\"\n",
    "        # self.datastr = \"ogbn-arxiv\"\n",
    "        # self.datastr = \"ogbn-papers100M\"\n",
    "        self.batch_size = 4096\n",
    "        self.epochs = 2000\n",
    "        self.hidden = 256\n",
    "        self.test = True\n",
    "        self.layer_norm = 1\n",
    "        self.dev = 0 # GPU ID\n",
    "        self.patience = 300\n",
    "        \n",
    "        if self.datastr in [\"flickr\", \"reddit\"]:\n",
    "            self.inductive = True\n",
    "        else:\n",
    "            self.inductive = False\n",
    "        \n",
    "        if self.model == \"GCN_LC\":\n",
    "            self.wd = 1e-4\n",
    "            self.lr = 0.001\n",
    "            self.dropout = 0\n",
    "            self.layer = 2\n",
    "        elif self.model == \"JKnet_LC\":\n",
    "            self.wd = 1e-6\n",
    "            self.lr = 1e-4\n",
    "            self.dropout = 0.5\n",
    "            self.layer = 3\n",
    "            self.pooling = \"concat\"\n",
    "        elif self.model == \"GPRGNN_LC4\":\n",
    "            self.wd = 1e-8\n",
    "            self.lr = 0.001\n",
    "            self.dropout = 0.5\n",
    "            self.layer = 5\n",
    "            self.alpha = 0.9\n",
    "        elif self.model == \"SGC\":\n",
    "            self.wd = 1e-9\n",
    "            self.lr = 0.01\n",
    "            self.layer = 2\n",
    "        elif self.model == \"FSGNN\":\n",
    "            # FSGNN\n",
    "            self.layer = 3\n",
    "            self.wd1 = 1e-05\n",
    "            self.wd2 = 1e-06\n",
    "            self.wd3 = 9e-06\n",
    "            self.wd_att = 0.1\n",
    "            self.lr1 = 5e-04\n",
    "            self.lr2 = 0.001\n",
    "            self.lr3 = 0.001\n",
    "            self.lr_att = 0.0005\n",
    "            self.dp1 = 0.5\n",
    "            self.dp2 = 0.6\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "num_layer = args.layer\n",
    "layer_norm = bool(int(args.layer_norm))\n",
    "batch_size = args.batch_size\n",
    "total_filt = 2*num_layer + 1\n",
    "cudaid = \"cuda:\"+str(args.dev)\n",
    "device = torch.device(cudaid)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fb545-0c31-4455-9585-816a1a0a5bc7",
   "metadata": {},
   "source": [
    "# Precomputation of feature aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae237bc-8722-4aad-9c53-e57409879e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting undirected matrix...\n",
      "Saving unnormalized adjacency matrix\n",
      "adjacency : Normalizing matrix A...\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 51.41it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.91it/s]\n",
      "1hop finished\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "2hop finished\n",
      "GPU max memory usage: 0.580328448\n"
     ]
    }
   ],
   "source": [
    "!python process_spmm.py --nhop $args.layer --datastr $args.datastr\n",
    "if args.model == \"FSGNN\":\n",
    "    !python process_spmm.py --filter exact1hop --nhop $args.layer --datastr $args.datastr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b47441-5aef-423e-9a8f-fcc1c5c5e2e5",
   "metadata": {},
   "source": [
    "## For inductive settings\n",
    "\n",
    "Note that you need to run the above cell even if args.inductive==True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038324ee-7319-487b-8806-3f96b8edd1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting undirected matrix...\n",
      "Saving unnormalized adjacency matrix\n",
      "adjacency : Normalizing matrix A...\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 92.12it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "1hop finished\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "2hop finished\n",
      "GPU max memory usage: 0.282274304\n"
     ]
    }
   ],
   "source": [
    "if args.inductive:\n",
    "    !python process_spmm.py --nhop $args.layer --datastr $args.datastr --inductive True\n",
    "    if args.model == \"FSGNN\":\n",
    "        !python process_spmm.py --filter exact1hop --nhop $args.layer --datastr $args.datastr --inductive True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec5bc1-4917-4a5e-be3c-18bc0429ef3a",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27931115-db6d-4223-b427-357a2d431675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 pretrained/5db664dbeb674ec9943a6943b879259a.pt\n"
     ]
    }
   ],
   "source": [
    "data_path = './precomputation_data/'+args.datastr\n",
    "\n",
    "#### Load node features used for input model ####\n",
    "if args.model == \"FSGNN\" or args.model == \"JKnet_LC\":\n",
    "    if args.model == \"FSGNN\":\n",
    "        filters = ['adjacency', 'exact1hop']\n",
    "    elif args.model == \"JKnet_LC\":\n",
    "        filters = ['adjacency']\n",
    "    # training data\n",
    "    train_data = []\n",
    "    with open(data_path+'/feature_training.pickle',\"rb\") as fopen:\n",
    "        train_data.append(pickle.load(fopen))\n",
    "    for filt in filters:\n",
    "        for i in range(1,args.layer+1):\n",
    "            with open(data_path+'/'+filt+'_'+str(i)+\"_training.pickle\",\"rb\") as fopen:\n",
    "                train_data.append(pickle.load(fopen))\n",
    "    # validation data\n",
    "    valid_data = []\n",
    "    with open(data_path+'/feature_validation.pickle',\"rb\") as fopen:\n",
    "        valid_data.append(pickle.load(fopen))\n",
    "    for filt in filters:\n",
    "        for i in range(1,args.layer+1):\n",
    "            with open(data_path+'/'+filt+'_'+str(i)+\"_validation.pickle\",\"rb\") as fopen:\n",
    "                valid_data.append(pickle.load(fopen))\n",
    "    # test data\n",
    "    test_data = []\n",
    "    with open(data_path+'/feature_test.pickle',\"rb\") as fopen:\n",
    "        test_data.append(pickle.load(fopen))\n",
    "    for filt in filters:\n",
    "        for i in range(1,args.layer+1):\n",
    "            with open(data_path+'/'+filt+'_'+str(i)+\"_test.pickle\",\"rb\") as fopen:\n",
    "                test_data.append(pickle.load(fopen))\n",
    "elif args.model == \"SGC\" or args.model == \"GCN_LC\":\n",
    "    # training data\n",
    "    with open(data_path+\"/adjacency_2_training.pickle\",\"rb\") as fopen:\n",
    "        train_data=[pickle.load(fopen)]\n",
    "    # validation data\n",
    "    with open(data_path+\"/adjacency_2_validation.pickle\",\"rb\") as fopen:\n",
    "        valid_data=[pickle.load(fopen)]\n",
    "    # test data\n",
    "    with open(data_path+\"/adjacency_2_test.pickle\",\"rb\") as fopen:\n",
    "        test_data=[pickle.load(fopen)]\n",
    "elif args.model == \"GPRGNN_LC3\" or args.model == \"GPRGNN_LC4\":\n",
    "    # training data\n",
    "    train_data = []\n",
    "    with open(data_path+'/feature_training.pickle',\"rb\") as fopen:\n",
    "        train_data.append(pickle.load(fopen))\n",
    "    for i in range(1,args.layer+1):\n",
    "        with open(data_path+'/adjacency_'+str(i)+\"_training.pickle\",\"rb\") as fopen:\n",
    "            train_data.append(pickle.load(fopen))\n",
    "    # validation data\n",
    "    valid_data = []\n",
    "    with open(data_path+'/feature_validation.pickle',\"rb\") as fopen:\n",
    "        valid_data.append(pickle.load(fopen))\n",
    "    for i in range(1,args.layer+1):\n",
    "        with open(data_path+'/adjacency_'+str(i)+\"_validation.pickle\",\"rb\") as fopen:\n",
    "            valid_data.append(pickle.load(fopen))\n",
    "    # test data\n",
    "    test_data = []\n",
    "    with open(data_path+'/feature_test.pickle',\"rb\") as fopen:\n",
    "        test_data.append(pickle.load(fopen))\n",
    "    for i in range(1,args.layer+1):\n",
    "        with open(data_path+'/adjacency_'+str(i)+\"_test.pickle\",\"rb\") as fopen:\n",
    "            test_data.append(pickle.load(fopen))\n",
    "\n",
    "with open(data_path+\"/labels.pickle\",\"rb\") as fopen:\n",
    "    labels = pickle.load(fopen)\n",
    "\n",
    "train_labels = labels[0].reshape(-1).long().to(device)\n",
    "valid_labels = labels[1].reshape(-1).long().to(device)\n",
    "test_labels = labels[2].reshape(-1).long().to(device)\n",
    "\n",
    "num_features = train_data[0].shape[1]\n",
    "num_labels = max(int(train_labels.max()), int(valid_labels.max()), int(test_labels.max())) + 1\n",
    "checkpt_file = 'pretrained/'+uuid.uuid4().hex+'.pt'\n",
    "print(cudaid,checkpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49042fc-160e-4fb8-88a1-7af73097cbde",
   "metadata": {},
   "source": [
    "# Define functions for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8579b41-c35a-465a-8186-b158237de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(input_data):\n",
    "    num_sample = input_data[0].shape[0]\n",
    "    list_bat = []\n",
    "    for i in range(0,num_sample,batch_size):\n",
    "        if (i+batch_size)<num_sample:\n",
    "            list_bat.append((i,i+batch_size))\n",
    "        else:\n",
    "            list_bat.append((i,num_sample))\n",
    "    return list_bat\n",
    "\n",
    "def test(st,end):\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        output = model(test_data,layer_norm,device,st,end)\n",
    "        loss_test = F.nll_loss(output, test_labels[st:end])\n",
    "        acc_test = accuracy(output, test_labels[st:end],batch=True)\n",
    "        return loss_test.item(),acc_test.item()\n",
    "\n",
    "def train_step(model,optimizer):\n",
    "    def train(st,end):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_data,layer_norm,device,st,end)\n",
    "        acc_train = accuracy(output, train_labels[st:end])\n",
    "        loss_train = F.nll_loss(output, train_labels[st:end])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        return loss_train.item(),acc_train.item()\n",
    "    def validate(st,end):\n",
    "        model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            output = model(valid_data,layer_norm,device,st,end)\n",
    "            loss_val = F.nll_loss(output, valid_labels[st:end])\n",
    "            acc_val = accuracy(output, valid_labels[st:end],batch=True)\n",
    "            return loss_val.item(),acc_val.item()\n",
    "        \n",
    "    bad_counter = 0\n",
    "    best = 999999999\n",
    "    best_epoch = 0\n",
    "    acc = 0\n",
    "    valid_num = valid_data[0].shape[0]\n",
    "    for epoch in range(args.epochs):\n",
    "        list_loss = []\n",
    "        list_acc = []\n",
    "        random.shuffle(list_bat_train)\n",
    "        for st,end in list_bat_train:\n",
    "            loss_tra,acc_tra = train(st,end)\n",
    "            list_loss.append(loss_tra)\n",
    "            list_acc.append(acc_tra)\n",
    "        loss_tra = np.round(np.mean(list_loss),4)\n",
    "        acc_tra = np.round(np.mean(list_acc),4)\n",
    "\n",
    "        list_loss_val = []\n",
    "        list_acc_val = []\n",
    "        for st,end in list_bat_val:\n",
    "            loss_val,acc_val = validate(st,end)\n",
    "            list_loss_val.append(loss_val)\n",
    "            list_acc_val.append(acc_val)\n",
    "\n",
    "        loss_val = np.mean(list_loss_val)\n",
    "        acc_val = (np.sum(list_acc_val))/valid_num\n",
    "        \n",
    "        #Uncomment to see losses\n",
    "        if(epoch+1)%50 == 0:\n",
    "            print('Epoch:{:04d}'.format(epoch+1),\n",
    "                'train',\n",
    "                'loss:{:.3f}'.format(loss_tra),\n",
    "                'acc:{:.2f}'.format(acc_tra*100),\n",
    "                '| val',\n",
    "                'loss:{:.3f}'.format(loss_val),\n",
    "                'acc:{:.2f}'.format(acc_val*100))\n",
    "        if loss_val < best:\n",
    "            best = loss_val\n",
    "            best_epoch = epoch\n",
    "            acc = acc_val\n",
    "            best_acc_tra = acc_tra\n",
    "            torch.save(model.state_dict(), checkpt_file)\n",
    "            best_model = model\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        if bad_counter == args.patience:\n",
    "            break\n",
    "    return acc, best, best_epoch, best_model, best_acc_tra, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f6ab8-5cd7-4ecd-bbe7-4d0268445aa7",
   "metadata": {},
   "source": [
    "# Model setting and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81300f0e-34c8-44c2-85f0-b622671d61cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0050 train loss:1.412 acc:49.99 | val loss:1.428 acc:51.99\n",
      "Epoch:0100 train loss:1.299 acc:53.71 | val loss:1.379 acc:52.22\n",
      "Epoch:0150 train loss:1.194 acc:57.44 | val loss:1.368 acc:51.94\n",
      "Epoch:0200 train loss:1.091 acc:61.24 | val loss:1.373 acc:51.64\n",
      "Epoch:0250 train loss:0.993 acc:65.22 | val loss:1.384 acc:50.93\n",
      "Epoch:0300 train loss:0.918 acc:68.00 | val loss:1.407 acc:50.94\n",
      "Epoch:0350 train loss:0.841 acc:70.80 | val loss:1.421 acc:50.42\n",
      "Epoch:0400 train loss:0.758 acc:74.65 | val loss:1.440 acc:50.54\n",
      "Epoch:0450 train loss:0.756 acc:74.21 | val loss:1.480 acc:48.51\n",
      "training time: 27.054322719573975\n",
      "best valid accuracy: 0.5193617784152026\n",
      "test valid accuracy: 0.4878770223636445\n"
     ]
    }
   ],
   "source": [
    "list_bat_train = create_batch(train_data)\n",
    "list_bat_val = create_batch(valid_data)\n",
    "list_bat_test = create_batch(test_data)\n",
    "\n",
    "if args.model == \"GCN_LC\":\n",
    "    model = GCN_LC(nfeat=num_features,\n",
    "                   nclass=num_labels,\n",
    "                   nhidden=args.hidden,\n",
    "                   dropout=args.dropout\n",
    "                  ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.wd)\n",
    "elif args.model == \"JKnet_LC\":\n",
    "    model = JKnet_LC(nfeat=num_features,\n",
    "                     nlayers=args.layer,\n",
    "                     nclass=num_labels,\n",
    "                     nhidden=args.hidden,\n",
    "                     dropout=args.dropout,\n",
    "                     pooling=args.pooling\n",
    "                    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.wd)\n",
    "elif args.model == \"GPRGNN_LC4\":\n",
    "    model = GPRGNN_LC4(nfeat=num_features,\n",
    "                      nlayers=args.layer,\n",
    "                      nhidden=args.hidden,\n",
    "                      nclass=num_labels,\n",
    "                      dropout=0.5,\n",
    "                      dp=args.dropout,\n",
    "                      alpha=args.alpha,\n",
    "                     ).to(device)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.lin1.parameters(), 'weight_decay': args.wd, 'lr': args.lr},\n",
    "        {'params': model.lin2.parameters(), 'weight_decay': args.wd, 'lr': args.lr},\n",
    "        {'params': model.lin3.parameters(), 'weight_decay': args.wd, 'lr': args.lr},\n",
    "        {'params': model.lin4.parameters(), 'weight_decay': args.wd, 'lr': args.lr},\n",
    "        {'params': model.temp,'weight_decay': 0.0, 'lr': args.lr}\n",
    "        ])\n",
    "elif args.model == \"SGC\":\n",
    "    model = SGC(nfeat=num_features,\n",
    "               nclass=num_labels).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.wd)\n",
    "elif args.model == \"FSGNN\":\n",
    "    model = FSGNN_Large(nfeat=num_features,\n",
    "                nlayers=2*args.layer + 1,\n",
    "                nhidden=args.hidden,\n",
    "                nclass=num_labels,\n",
    "                dp1=args.dp1,dp2=args.dp2).to(device)\n",
    "    optimizer_sett = [\n",
    "        {'params': model.wt1.parameters(), 'weight_decay': args.wd1, 'lr': args.lr1},\n",
    "        {'params': model.fc2.parameters(), 'weight_decay': args.wd2, 'lr': args.lr2},\n",
    "        {'params': model.fc3.parameters(), 'weight_decay': args.wd3, 'lr': args.lr3},\n",
    "        {'params': model.att, 'weight_decay': args.wd_att, 'lr': args.lr_att},\n",
    "        ]\n",
    "    optimizer = optim.Adam(optimizer_sett)\n",
    "\n",
    "# train\n",
    "t_start = time.time()\n",
    "val_acc, best, best_epoch, best_model, train_acc, epochs_stopped = train_step(model,optimizer)\n",
    "t_train = time.time() - t_start\n",
    "print(\"training time:\", t_train)\n",
    "\n",
    "# test\n",
    "test_num = test_data[0].shape[0]\n",
    "list_loss_test = []\n",
    "list_acc_test = []\n",
    "model = best_model\n",
    "for st,end in list_bat_test:\n",
    "    loss_test,acc_test = test(st,end)\n",
    "    list_loss_test.append(loss_test)\n",
    "    list_acc_test.append(acc_test)\n",
    "test_acc = (np.sum(list_acc_test))/test_num\n",
    "\n",
    "print(\"best valid accuracy:\", val_acc)\n",
    "print(\"test valid accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bee7a-df83-4965-9fad-7e8810457781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
